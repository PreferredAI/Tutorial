{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09_deep_learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLXWx4uhqtYJ",
        "colab_type": "text"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/PreferredAI/tutorials/blob/master/recommender-systems/09_deep_learning.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/PreferredAI/tutorials/blob/master/recommender-systems/09_deep_learning.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzPrmZOMtXt1",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh3gYub8f6qZ",
        "colab_type": "text"
      },
      "source": [
        "Recently deep learning has produced significant advancements on various machine learning tasks. To develop an appreciation of how deep learning has an effect on recommender systems, in this tutorial we look into example models from two broad categories. One models collaborative filtering or user-item interaction data, while the other models content associated with items (or users)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqktDa7H2hKz",
        "colab_type": "text"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41oWCMUG2eC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --quiet cornac==1.7.1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqKrDcGH2k7E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ac9e0246-3b29-43bc-8c62-7dc27026bfbf"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import cornac\n",
        "from cornac.data import Reader\n",
        "from cornac.datasets import movielens, citeulike\n",
        "from cornac.eval_methods import RatioSplit\n",
        "from cornac.models import GMF, MLP, NeuMF, VAECF, CDL, ConvMF, WMF, CTR \n",
        "from cornac.data import TextModality\n",
        "from cornac.data.text import BaseTokenizer\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "print(f\"System version: {sys.version}\")\n",
        "print(f\"Cornac version: {cornac.__version__}\")\n",
        "print(f\"Tensorflow version: {tf.__version__}\")\n",
        "\n",
        "SEED = 42\n",
        "VERBOSE = True"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "System version: 3.6.9 (default, Apr 18 2020, 01:56:04) \n",
            "[GCC 8.4.0]\n",
            "Cornac version: 1.7.1\n",
            "Tensorflow version: 1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r76coF3ZyWxb",
        "colab_type": "text"
      },
      "source": [
        "## 2. Deep Learning for Collaborative Filtering\n",
        "\n",
        "In this section, we study two frameworks of deep learning, and how they can be used to model user-item interactions.  Neural collaborative filtering is based on multi-layer perceptrons, while as suggested by its name, variational autoencoder for collaborative filtering is based on VAE.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GePqE99j5nr",
        "colab_type": "text"
      },
      "source": [
        "### Neural Collaborative Filtering\n",
        "\n",
        "Neural collaborative filtering consists of a family of models developed based on neural networks to tackle the problem of collaborative filtering based on implicit feedback.  The final model NeuMF is a combination of two components, namely Generalized Matrix Factorization (GMF) and Multi-Layer Perceptrons (MLP), which are also independent models respectively.\n",
        "\n",
        "- GMF projects element-wise product between user and item embedding vectors through *Sigmoid* function to out the prediction:\n",
        "\\begin{equation}\n",
        "\\hat{y}_{ij} = \\sigma \\big(\\mathbf{w}^T (\\mathbf{u}_i \\odot \\mathbf{v}_j)\\big) = \\sigma \\big(\\sum_{k=1}^{K} w_k . u_{ik} . v_{jk}\\big)\n",
        "\\end{equation}\n",
        "where $\\odot$ is element-wise product, $\\mathbf{u}_i$ and $\\mathbf{v}_j$ are user and item embeddings, $\\mathbf{w}$ are the projection weights, and $\\sigma$ is the *Sigmoid* function.\n",
        "\n",
        "- MLP projects concatenation of user and item embeddings through multiple layers:\n",
        "\\begin{align}\n",
        "\\mathbf{z}_1 & = \\phi_1 (\\mathbf{u}_i, \\mathbf{v}_j) = \\begin{bmatrix} \\mathbf{u}_i\\\\ \\mathbf{v}_j \\end{bmatrix} \\\\\n",
        "\\mathbf{z}_2 & = \\phi_2 (\\mathbf{w}_2^T \\mathbf{z}_1 + b_2) \\\\\n",
        "\\vdots & \\\\\n",
        "\\mathbf{z}_L & = \\phi_L (\\mathbf{w}_L^T \\mathbf{z}_{L-1} + b_L) \\\\\n",
        "\\hat{y}_{ij} & = \\sigma (\\mathbf{w}_{L+1}^T \\mathbf{z}_L) \n",
        "\\end{align}\n",
        "where $\\phi$ is a non-linear activation function (could be *Tanh* as recommended by the authors).\n",
        "\n",
        "- NeuMF is a fusion of GMF and MLP:\n",
        "\\begin{equation}\n",
        "\\hat{y}_{ij} = \\sigma \\bigg(\\mathbf{w}^T \\begin{bmatrix} \\mathbf{u}_i \\odot \\mathbf{v}_j \\\\ \\mathbf{z}_L \\end{bmatrix} \\bigg)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "The models are learned by minimizing log-loss function:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathcal{L}  & = - \\sum_{(u,i) \\in \\mathbf{R}^+} \\log \\hat{y}_{ui} - \\sum_{(u,i) \\in \\mathbf{R}^-} \\log (1 - \\hat{y}_{ui}) \\\\\n",
        "& = - \\sum_{(u,i) \\in \\mathbf{R}^+ \\cup \\mathbf{R}^- } y_{ui} \\log \\hat{y}_{ui} + (1 - y_{ui}) \\log (1 - \\hat{y}_{ui})\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "Let's try the models all together! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2iyzuFFs1Fq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "2efb0200-5e2d-4674-e766-353c0663cb83"
      },
      "source": [
        "GMF_FACTORS = 8  # @param\n",
        "MLP_LAYERS = [32, 16, 8]  # @param\n",
        "ACTIVATION = \"tanh\"  # @param [\"tanh\", \"sigmoid\", \"relu\", \"leaky_relu\"]\n",
        "NEG_SAMPLES = 3  # @param\n",
        "NUM_EPOCHS = 10  # @param \n",
        "BATCH_SIZE = 256  # @param\n",
        "LEARNING_RATE = 0.001  # @param\n",
        "\n",
        "gmf = GMF(num_factors=GMF_FACTORS, num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, \n",
        "          num_neg=NEG_SAMPLES, lr=LEARNING_RATE, seed=SEED, verbose=VERBOSE)\n",
        "mlp = MLP(layers=MLP_LAYERS, act_fn=ACTIVATION, num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, \n",
        "          num_neg=NEG_SAMPLES, lr=LEARNING_RATE, seed=SEED, verbose=VERBOSE)\n",
        "neumf = NeuMF(num_factors=GMF_FACTORS, layers=MLP_LAYERS, act_fn=ACTIVATION, num_epochs=NUM_EPOCHS,\n",
        "              num_neg=NEG_SAMPLES, batch_size=BATCH_SIZE, lr=LEARNING_RATE, seed=SEED, verbose=VERBOSE)\n",
        "\n",
        "wmf = WMF(k=GMF_FACTORS, max_iter=200, learning_rate=0.001, seed=SEED, verbose=VERBOSE)\n",
        "\n",
        "ml_100k = movielens.load_feedback(variant=\"100K\", reader=Reader(bin_threshold=4.0))\n",
        "ratio_split = RatioSplit(\n",
        "  data=ml_100k, test_size=0.2, exclude_unknowns=True, seed=SEED, verbose=VERBOSE\n",
        ")\n",
        "ndcg_50 = cornac.metrics.NDCG(k=50)\n",
        "rec_50 = cornac.metrics.Recall(k=50)\n",
        "\n",
        "cornac.Experiment(\n",
        "  eval_method=ratio_split,\n",
        "  models=[gmf, mlp, neumf, wmf],\n",
        "  metrics=[ndcg_50, rec_50],\n",
        ").run()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TEST:\n",
            "...\n",
            "      | NDCG@50 | Recall@50 | Train (s) | Test (s)\n",
            "----- + ------- + --------- + --------- + --------\n",
            "GMF   |  0.1609 |    0.2778 |   43.2287 |   1.4973\n",
            "MLP   |  0.1630 |    0.2807 |   42.7741 |   1.5705\n",
            "NeuMF |  0.2412 |    0.4268 |   43.8981 |   1.7990\n",
            "WMF   |  0.1976 |    0.3753 |    8.4105 |   0.7220\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUFnKYm-uWLh",
        "colab_type": "text"
      },
      "source": [
        "### Variational Autoencoder for Collaborative Filtering (VAECF)\n",
        "\n",
        "\n",
        "Variational Autoencoders (VAE) is a type of autoencoders which is a neural network with auto-associative mapping of inputs.  Normal autoencoders learns a determinisic latent representation of an input, while VAE learns a distribution of that representation.  VAECF model extends VAE for the collaborative filtering problem with implicit feedback data.\n",
        "\n",
        "Let the data for user $i$ (e.g., history clicks) be a vector $\\mathbf{x}_i \\in \\mathbb{N}^M$ where M is the number of items.  The generative model of VAECF can be described as follows:\n",
        "\n",
        "- User latent representation is sampled from standard Gaussion prior: $\\mathbf{u}_i \\sim \\mathcal{N}(0, \\mathbf{I}_K)$\n",
        "\n",
        "- $\\mathbf{z}_i$ is transformed via a non-linear function $f_{\\theta}$ to produce a probability distribution over items (normalized using *softmax*): $\\pi(\\mathbf{u}_i) \\propto \\exp \\{ f_{\\theta}(\\mathbf{u}_i) \\}$\n",
        "\n",
        "- User data $\\mathbf{x}_i$ is assumed to have been drawn from the multinomial distribution: $\\mathbf{x}_i \\sim \\mathrm{Mult}(N_i, \\pi(\\mathbf{u}_i))$\n",
        "\n",
        "The Multinomial (*mult*) log-likelihood for user $i$ conditioned on the latent representation is:\n",
        "\n",
        "\\begin{equation}\n",
        "\\log p_{\\theta} (\\mathbf{x}_i | \\mathbf{u}_i) = \\mathcal{L}_{mult} = \\sum_{i=1}^{N} \\sum_{j=1}^{M} x_{ij} \\log \\pi_j(\\mathbf{u}_i)\n",
        "\\end{equation}\n",
        "\n",
        "Other choices of likelihood including Bernoulli (*bern*), Gaussian (*gaus*), or Poisson (*pois*) can be used:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathcal{L}_{bern} &= \\sum_{i=1}^{N} \\sum_{j=1}^{M} x_{ij} \\log \\sigma(f_{\\theta}(\\mathbf{u}_i)_j) + (1 - x_{ij}) \\log (1 - \\sigma(f_{\\theta}(\\mathbf{u}_i)_j)) \\\\\n",
        "\\mathcal{L}_{gaus} &= \\sum_{i=1}^{N} \\sum_{j=1}^{M} - (x_{ij} - \\sigma(f_{\\theta}(\\mathbf{u}_i)_j))^2  \\\\\n",
        "\\mathcal{L}_{pois} &= \\sum_{i=1}^{N} \\sum_{j=1}^{M} x_{ij} \\log \\sigma(f_{\\theta}(\\mathbf{u}_i)_j) - \\sigma(f_{\\theta}(\\mathbf{u}_i)_j) \\\\\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n",
        "The encoder-decoder architecture is due to the use of amortized inference, a technique of variational inference, for learning the generative model (**decoder**) $f_{\\theta}$.  Amortized inference learns an inference model (**encoder**) $g_{\\phi} (\\mathbf{x}_i) \\equiv [\\mu_{\\phi}(\\mathbf{x}_i), \\sigma_{\\phi}(\\mathbf{x}_i)]$, which makes use of the observed data $\\mathbf{x}_i$ to approximate the true intractable posterior $p(\\mathbf{u}_i | \\mathbf{x}_i)$ with a simpler variational distribution:\n",
        "\n",
        "\\begin{equation}\n",
        "q_{\\phi}(\\mathbf{u}_i | \\mathbf{x}_i) = \\mathcal{N} (\\mu_{\\phi}(\\mathbf{x}_i), \\mathrm{diag}\\{\\sigma_{\\phi}^2(\\mathbf{x}_i)\\}) \n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Learning the model parameters $\\{\\theta, \\phi\\}$ is done via maximizing the evidence lower bound (ELBO):\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathcal{L}_{\\beta} = \\mathbb{E}_{q_{\\phi} (\\mathbf{u}_i | \\mathbf{x}_i)} [\\log p_{\\theta} (\\mathbf{x}_i | \\mathbf{u}_i)] - \\beta . \\mathrm{KL}(q_{\\phi} (\\mathbf{u}_i | \\mathbf{x}_i) || p(\\mathbf{u}_i))\n",
        "\\end{equation}\n",
        "\n",
        "where $\\beta$ is a hyper-parameter to control the strength of regularization introduced by enforcing small Kullback-Leibler (KL) divergence between posterior $q_{\\phi} (\\mathbf{u}_i | \\mathbf{x}_i)$ and the prior $p(\\mathbf{u}_i)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndiIw1WJ8Ghv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "3e77890a-108c-4877-c03c-3864c092f513"
      },
      "source": [
        "NUM_FACTORS = 25  # @param\n",
        "AE_LAYERS = [100, 50]  # @param\n",
        "ACTIVATION = \"tanh\"  # @param [\"tanh\", \"sigmoid\", \"relu\", \"leaky_relu\"]\n",
        "LIKELIHOOD = \"bern\"  # @param [\"bern\", \"mult\", \"gaus\", \"pois\"]\n",
        "NUM_EPOCHS = 600  # @param \n",
        "BATCH_SIZE = 256  # @param\n",
        "LEARNING_RATE = 0.001  # @param\n",
        "\n",
        "vaecf = VAECF(k=NUM_FACTORS, autoencoder_structure=AE_LAYERS, act_fn=ACTIVATION,\n",
        "              likelihood=LIKELIHOOD, n_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\n",
        "              learning_rate=LEARNING_RATE, seed=SEED, verbose=VERBOSE, use_gpu=True)\n",
        "\n",
        "wmf = WMF(k=NUM_FACTORS, max_iter=200, learning_rate=0.001, seed=SEED, verbose=VERBOSE)\n",
        "\n",
        "ml_100k = movielens.load_feedback(variant=\"100K\", reader=Reader(bin_threshold=4.0))\n",
        "ratio_split = RatioSplit(\n",
        "  data=ml_100k, test_size=0.2, exclude_unknowns=True, seed=SEED, verbose=VERBOSE\n",
        ")\n",
        "\n",
        "cornac.Experiment(\n",
        "  eval_method=ratio_split, models=[vaecf, wmf], metrics=[rec_50, ndcg_50],\n",
        ").run()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TEST:\n",
            "...\n",
            "      | NDCG@50 | Recall@50 | Train (s) | Test (s)\n",
            "----- + ------- + --------- + --------- + --------\n",
            "VAECF |  0.2405 |    0.4250 |   29.1858 |   1.6399\n",
            "WMF   |  0.2045 |    0.3958 |    8.8830 |   0.7460\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8iXcfuKsVa8",
        "colab_type": "text"
      },
      "source": [
        "## 3. Deep Learning for Content-Based Filtering\n",
        "\n",
        "A different way to use deep learning is to model the content of items (or users).  The key idea is to derive low-dimensional representations from this content, originally in the form of text, images, etc., and to use these representations to enhance recommender systems in a similar way to content-based methods.  In the following, we illustrate two examples based on autoencoder and convolutional neural networks respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "742yJ53Fj1Zz",
        "colab_type": "text"
      },
      "source": [
        "### Collaborative Deep Learning (CDL)\n",
        "\n",
        "CDL makes use of Stacked-Denoising Autoencoders (SDAE) to model item text content, with the objective of improving upon Collaborative Topic Regression (CTR) model making use of topic model for the similar purpose.  CDL is a hierarchical Bayesian model, the generative process is as follows:\n",
        "\n",
        "- For each user $i$\n",
        "  - Draw latent vector $\\mathbf{u}_i \\sim \\mathcal{N}(0, \\lambda^{-1} \\mathbf{I})$\n",
        "- For each item $j$\n",
        "  - Put its corrupted content $\\mathbf{x}_j^{0}$ through SDAE\n",
        "  - Draw clean content $\\mathbf{x}_j \\sim \\mathcal{N}(\\mathbf{x}_L, \\lambda^{-1}\\mathbf{I})$\n",
        "    - User middle representation $\\mathbf{x}_j^{L/2}$\n",
        "  - Draw item offset $\\mathbf{\\epsilon}_j \\sim \\mathcal{N}(0, \\lambda^{-1} \\mathbf{I})$\n",
        "  - Set item latent vector $\\mathbf{v}_j$ to be: $\\mathbf{v}_j = \\mathbf{\\epsilon}_j + \\mathbf{x}_j^{L/2}$\n",
        "- For each user-item pair $(i, j)$\n",
        "  - Draw rating $r_{ij} \\sim \\mathcal{N}(\\mathbf{u}_i^T \\mathbf{v}_j, c_{ij}^{-1})$\n",
        "\n",
        "The confidence parameter $c_{ij}$ is defined similar to that for CTR:\n",
        "\n",
        "\\begin{equation}\n",
        "c_{i,j} = \n",
        "\\begin{cases} \n",
        "a & \\mbox{if } r_{i,j} > 0 \\\\\n",
        "b & \\mbox{otherwise }\n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "Learning CDL model is done via minimizing the loss function:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathcal{L} = & \\frac{\\lambda_u}{2} \\sum_{i} || \\mathbf{u}_i ||_2^2 + \\frac{\\lambda_w}{2} \\sum_{l} (||\\mathbf{W}_l||_F^2 + ||\\mathbf{b}_l||^2) \\\\\n",
        "& + \\frac{\\lambda_v}{2} \\sum_{j} || \\mathbf{v}_j - \\mathbf{x}^{\\frac{L}{2}}_{j} ||_2^2 \\\\\n",
        "& + \\frac{\\lambda_n}{2} \\sum_{j} || \\mathbf{x}^{L}_{j} - \\mathbf{x}^{c}_{j} ||_2^2 \\\\\n",
        "& +  \\sum_{i,j} \\frac{c_{ij}}{2} (r_{ij} - \\mathbf{u}_i^T \\mathbf{v}_j)^2\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "Let's do a comparison between CDL and CTR models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ2Al6omrlEP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "ac62b452-feae-408f-b2f8-43e408e05d52"
      },
      "source": [
        "VOCAB_SIZE = 2000  # @param\n",
        "NUM_FACTORS = 50  # @param\n",
        "AE_LAYERS = [500, 100]  # @param\n",
        "ACTIVATION = \"tanh\"  # @param [\"tanh\", \"sigmoid\", \"relu\", \"leaky_relu\"]\n",
        "A = 1.0  # @param\n",
        "B = 0.01  # @param\n",
        "LAMBDA_U = 0.1  # @param\n",
        "LAMBDA_V = 0.1  # @param\n",
        "LAMBDA_W = 0.1  # @param\n",
        "LAMBDA_N = 10.0  # @param\n",
        "CORRUPTION_RATE = 0.3  # @param\n",
        "DROPOUT_RATE = 0.1  # @param\n",
        "NUM_EPOCHS = 100  # @param \n",
        "BATCH_SIZE = 100  # @param\n",
        "LEARNING_RATE = 0.001  # @param\n",
        "\n",
        "cdl = CDL(k=NUM_FACTORS, autoencoder_structure=AE_LAYERS, act_fn=ACTIVATION, a=A, b=B,\n",
        "          lambda_u=LAMBDA_U, lambda_v=LAMBDA_V, lambda_w=LAMBDA_W, lambda_n=LAMBDA_N,\n",
        "          corruption_rate=CORRUPTION_RATE, dropout_rate=DROPOUT_RATE, vocab_size=VOCAB_SIZE,\n",
        "          max_iter=NUM_EPOCHS, batch_size=BATCH_SIZE, seed=SEED, verbose=VERBOSE)\n",
        "\n",
        "ctr = CTR(k=NUM_FACTORS, max_iter=NUM_EPOCHS, a=A, b=B, lambda_u=LAMBDA_U, lambda_v=LAMBDA_V, seed=SEED, verbose=VERBOSE)\n",
        "\n",
        "plots, movie_ids = movielens.load_plot()\n",
        "feedback = movielens.load_feedback(variant=\"100k\", reader=Reader(item_set=movie_ids, bin_threshold=4.0))\n",
        "item_text_modality = TextModality(\n",
        "  corpus=plots, ids=movie_ids, tokenizer=BaseTokenizer(sep=\"\\t\", stop_words=\"english\"), max_vocab=VOCAB_SIZE,\n",
        ")\n",
        "\n",
        "ratio_split = RatioSplit(\n",
        "  data=feedback, \n",
        "  test_size=0.2, \n",
        "  exclude_unknowns=True, \n",
        "  item_text=item_text_modality, \n",
        "  seed=SEED, verbose=VERBOSE,\n",
        ")\n",
        "\n",
        "cornac.Experiment(eval_method=ratio_split, models=[cdl, ctr], metrics=[rec_50, ndcg_50]).run()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TEST:\n",
            "...\n",
            "    | NDCG@50 | Recall@50 | Train (s) | Test (s)\n",
            "--- + ------- + --------- + --------- + --------\n",
            "CDL |  0.2063 |    0.4066 |   25.0348 |   0.7186\n",
            "CTR |  0.1865 |    0.3343 |  117.5390 |   0.7346\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-8mCmABFe8e",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional Matrix Factorization (ConvMF)\n",
        "\n",
        "ConvMF model integrates convolutional neural network (CNN) into probabilistic matrix factorization (PMF).  The model captures contextual information of item documents (e.g., reviews, abstracts, or synopses) and further enhances the rating prediction accuracy.  The generative process is as follows:\n",
        "\n",
        "- For each user $i$\n",
        "  - Draw latent vector $\\mathbf{u}_i \\sim \\mathcal{N}(0, \\lambda^{-1} \\mathbf{I})$\n",
        "- For each item $j$\n",
        "  - Put its content $\\mathbf{x}_j$ through CNN parameterized by $\\mathbf{W}$\n",
        "  - Draw item offset $\\mathbf{\\epsilon}_j \\sim \\mathcal{N}(0, \\lambda^{-1} \\mathbf{I})$\n",
        "  - Set item latent vector $\\mathbf{v}_j$ to be: $\\mathbf{v}_j = \\mathbf{\\epsilon}_j + \\mathrm{CNN}(\\mathbf{W}, \\mathbf{x}_j)$\n",
        "- For each user-item pair $(i, j)$\n",
        "  - Draw rating $r_{ij} \\sim \\mathcal{N}(\\mathbf{u}_i^T \\mathbf{v}_j, \\sigma^2)$\n",
        "\n",
        "Learning ConvMF model is done via minimizing the following negative log-likelihood function:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathcal{L} = \\sum_{(i,j)\\in \\mathbf{R}} (r_{ij} - \\mathbf{u}_i^T \\mathbf{v}_j)^2 + \\lambda_u \\sum_{i}^{N} || \\mathbf{u}_i ||^2 + \\lambda_v \\sum_{j}^{M} || \\mathbf{v}_j - \\mathrm{CNN}(\\mathbf{W}, \\mathbf{x}_j)||^2 +\\lambda_\\mathbf{W} \\sum_{w_l \\in \\mathbf{W}} ||w_k||^2 \n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhO9P63eQwLs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "c3c2d39b-acea-4e66-8dc1-b0df78d237f4"
      },
      "source": [
        "VOCAB_SIZE = 2000  # @param\n",
        "NUM_FACTORS = 5  # @param\n",
        "HIDDEN_DIM = 50  # @param\n",
        "CNN_FILTER_SIZES = [2, 3, 4, 5]  # @param\n",
        "CNN_NUM_FILTERS = 128  # @param\n",
        "WORD_EMB_DIM = 32  # @param\n",
        "SEQ_MAX_LENGTH = 50  # @param\n",
        "DROPOUT_RATE = 0.2  # @param\n",
        "LAMBDA_U = 0.1  # @param\n",
        "LAMBDA_V = 0.1  # @param\n",
        "NUM_EPOCHS = 5  # @param\n",
        "CNN_EPOCHS = 3  # @param\n",
        "CNN_BATCH_SIZE = 256  # @param\n",
        "CNN_LEARNING_RATE = 0.001  # @param\n",
        "\n",
        "convmf = ConvMF(k=NUM_FACTORS, n_epochs=NUM_EPOCHS, cnn_epochs=CNN_EPOCHS, cnn_bs=CNN_BATCH_SIZE,\n",
        "                cnn_lr=CNN_LEARNING_RATE, emb_dim=WORD_EMB_DIM, max_len=SEQ_MAX_LENGTH,\n",
        "                filter_sizes=CNN_FILTER_SIZES, num_filters=CNN_NUM_FILTERS, hidden_dim=HIDDEN_DIM, \n",
        "                dropout_rate=DROPOUT_RATE, lambda_u=LAMBDA_U, lambda_v=LAMBDA_V,\n",
        "                seed=SEED, verbose=VERBOSE)\n",
        "\n",
        "plots, movie_ids = movielens.load_plot()\n",
        "feedback = movielens.load_feedback(variant=\"100k\", reader=Reader(item_set=movie_ids))\n",
        "item_text_modality = TextModality(\n",
        "  corpus=plots, ids=movie_ids, tokenizer=BaseTokenizer(sep=\"\\t\", stop_words=\"english\"), max_vocab=VOCAB_SIZE,\n",
        ")\n",
        "\n",
        "ratio_split = RatioSplit(\n",
        "  data=feedback, \n",
        "  test_size=0.2, \n",
        "  rating_threshold=4.0, \n",
        "  exclude_unknowns=True, \n",
        "  item_text=item_text_modality, \n",
        "  seed=SEED, \n",
        "  verbose=VERBOSE\n",
        ")\n",
        "\n",
        "rmse = cornac.metrics.RMSE()\n",
        "\n",
        "cornac.Experiment(\n",
        "  eval_method=ratio_split, models=[convmf], metrics=[rmse, rec_50], user_based=False\n",
        ").run()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TEST:\n",
            "...\n",
            "       |   RMSE | Recall@50 | Train (s) | Test (s)\n",
            "------ + ------ + --------- + --------- + --------\n",
            "ConvMF | 0.9795 |    0.0075 |   12.0318 |   0.7525\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FmiuTYiP_pB",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "1.   He, X., Liao, L., Zhang, H., Nie, L., Hu, X., & Chua, T. S. (2017, April). Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web (pp. 173-182).\n",
        "2.   Liang, D., Krishnan, R. G., Hoffman, M. D., & Jebara, T. (2018, April). Variational autoencoders for collaborative filtering. In Proceedings of the 2018 World Wide Web Conference (pp. 689-698).\n",
        "3.   Wang, H., Wang, N., & Yeung, D. Y. (2015, August). Collaborative deep learning for recommender systems. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1235-1244).\n",
        "4.   Kim, D., Park, C., Oh, J., Lee, S., & Yu, H. (2016, September). Convolutional matrix factorization for document context-aware recommendation. In Proceedings of the 10th ACM conference on recommender systems (pp. 233-240).\n",
        "5.   Cornac - A Comparative Framework for Multimodal Recommender Systems (https://cornac.preferred.ai/)"
      ]
    }
  ]
}